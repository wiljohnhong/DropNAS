%%%% ijcai20.tex

\typeout{IJCAI--PRICAI--20 Instructions for Authors}

% These are the instructions for authors for IJCAI-20.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai20.sty is NOT the same than previous years'
\usepackage{ijcai20}


% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{subfigure}
\usepackage{floatrow}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage{blindtext}


\newcommand{\hong}[1]{\textcolor{blue}{\textsf{[#1]}}}

\newcommand{\fig}[1]{Fig.~\ref{#1}}
\newcommand{\tab}[1]{Table.~\ref{#1}}

\newcommand{\weinan}[1]{{\bf \color{blue} [[Weinan says ``#1'']]}}
\newcommand{\citet}[1]{\citeauthor{#1}~\shortcite{#1}}

% Include other packages here, before hyperref.

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{DropNAS: Grouped Operation Dropout for Differentiable Architecture Search}

% Single author syntax
\author{
	Paper ID: 4586
	% Christian Bessiere
	% \affiliations
	% CNRS, University of Montpellier, France
	% \emails
	% pcchair@ijcai20.org
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% Check the ijcai20-multiauthor.tex file for detailed instructions
\iffalse
\author{
	First Author$^1$
	\and
	Second Author$^2$\and
	Third Author$^{2,3}$\And
	Fourth Author$^4$
	\affiliations
	$^1$First Affiliation\\
	$^2$Second Affiliation\\
	$^3$Third Affiliation\\
	$^4$Fourth Affiliation
	\emails
	\{first, second\}@example.com,
	third@other.example.com,
	fourth@example.com
}
\fi

\begin{document}

	\maketitle

	\begin{abstract}
		Neural architecture search (NAS) has shown encouraging results in automating the architecture design. Recently, searching on a one-shot model that leverages weight-sharing for cost reduction of NAS has attract attention.
		However, such a training process of a one-shot model is always unstable and the existing search paradigms on a one-shot model are still based on heuristics.
		On one hand, it is unclear that to what extent the parameters should be shared so that the stand-alone model can be precisely ranked according to their real performance. On the other hand, despite the efficiency advantage, one-level optimization for the weight-sharing model practically obtains worse performance than the bi-level approach.
		In this paper, we make a deep analysis on the collinearity problem among the correlated operations as pointed out in previous works, and find that there also exists Matthew Effect in one-level DARTS among the parameterized operations. Both the problems harm the performance in one-level differentiable architecture search (DARTS), which can be addressed by dropout techniques.
		Then we propose a novel grouped operation dropout algorithm named DropNAS to explore a better weight-sharing scheme, and adopt it into the DARTS search space to improve the one-level optimization. Extensive experiments are conducted, which demonstrate that DropNAS solves the above issues and achieves promising performance. Specifically, DropNAS achieves 2.26\% test error on CIFAR-10, 16.39\% on CIFAR-100 and 23.4\% on ImageNet. It is also observed that the best weight-sharing scheme found by DropNAS is robust across many search spaces on different datasets.
	\end{abstract}

	\section{Introduction}

	With the rapid growth of deep learning technology in recent years \cite{krizhevsky2012imagenet,silver2017mastering}, designing high-performance neural network architecture is attaining increasing attention. However, such architecture designing processes involve abundant human expertise. More recently, automatic neural architecture search (NAS) has been brought into focus and achieves state-of-the-art results on various tasks including \cite{zoph2016neural,luo2018neural,real2019regularized}, object detection \cite{ghiasi2019fpn,zoph2018learning} and recommender systems \cite{joglekar2019neural}, outperforming human-designed architectures.

	%The early works of NAS which apply reinforcement learning \cite{zoph2016neural} or evolution strategy \cite{real2019regularized} as their searching techniques require training each candidate architecture from scratch in order to obtain a feedback to guide the exploration, which cost vast computational resources, and therefore lead to a paradigm shift to searching efficiently on a weight-sharing one-shot model \cite{brock2017smash, liu2018darts, cai2018proxylessnas, pham2018efficient}.
	Currently one popular search strategy of NAS is to search on a weight-sharing model, where a supernet containing all the underlying subnets is built, each of them is optimized as the supernet trained \cite{brock2017smash,liu2018darts,cai2018proxylessnas,pham2018efficient}. The target architecture can be induced by just requiring to train the supernet once, which strikingly cuts down the search cost.

	Till now, however, it is still unclear that which search scheme should be employed so that the shared weights can better represent the case when they are inherited by the subnets and thus reflect performance more accurately. ENAS \cite{pham2018efficient} and SNAS \cite{xie2018snas} sample a single child model to update in each mini-batch. Differentiable architecture search (DARTS) \cite{liu2018darts} relaxes the search space as continuous, and updates all the shared model weights together with the architecture parameters by gradient descent. Nonetheless, these search schemes actually are not significantly better than random search according to a recent study \cite{yang2019nas}. ProxylessNAS \cite{cai2018proxylessnas} once samples two paths as the subnet to update and shows promising results, but in essence, it is a tradeoff that sacrifices the capacity of the weight-sharing model to reduce the memory requirement. In this paper, we demonstrate that unlike SNAS, DARTS or ProxylessNAS, differentiable architecture search on one-shot model is stable when just a few operations are disabled.

	Another problem in weight-sharing model is that the more efficient one-level optimization usually gets worse results than the less efficient bi-level approach \cite{liu2018darts}. A recent work \cite{li2019stacnas} points out that the multiple collinearity problem among correlated operations causes this problem. We further perform a deep analysis on such a problem and find out that Matthew Effect \cite{adam2019understanding} also exists in the one-level DARTS. Following \cite{bender2018understanding}, which leverages operation dropout in a one-shot model without learning architecture parameters, we propose DropNAS for one-level DARTS, which is a grouped operation dropout algorithm with two useful techniques, i.e. \textit{$\alpha$-adjust} and \textit{partial-decay}, showing that the one-level DARTS can also achieve promising performance on various datasets.


	In our experiments, we first show the architectures discovered by DropNAS achieve a test accuracy of 97.74\% on CIFAR-10 and 83.61\% on CIFAR-100, without increasing channels or epochs than that in DARTS during the evaluation. Moreover, with additional data augmentations \begin{comment}\weinan{can we not say `tricks'?}}\end{comment}, DropNAS can achieve 98.12\% and 85.90\% accuracy on CIFAR-10 and CIFAR-100 respectively. When transferred to ImageNet, our approach reaches only 23.4\% test error. Our approach indeed mitigates the Matthew Effect and multiple collinearity problem, and is transferable to other search spaces.

	In summary, our contributions can be listed as follows:
	\begin{itemize}
		\item We study the behavior of operations in weight-sharing model, and with the multiple collinearity problems discovered in previous work, we also observe Matthew Effect in one-level DARTS that harms the performance.
		\item We introduce grouped operation dropout into one-level DARTS to avoid these issues, together with \textit{$\alpha$-adjust} and \textit{partial-decay} techniques to prevent the passive update and unnecessary weight decay.
		\item We conduct sufficient experiments which show some state-of-the-art performance on various benchmark datasets, demonstrating that dropping just few operations can better stabilize the differentiable architecture search on one-shot model. We also show that the found search scheme is robust across various search spaces and datasets.
		\begin{comment}\weinan{merge the third and fourth bullets and write it in a more technical way.}\end{comment}
	\end{itemize}


	\section{Methodology}

	\subsection{Background: DARTS}

	In this work, we follow the DARTS framework \cite{liu2018darts}, whose objective is to figure out the best cell to be stacked to form the search space. For each cell we usually define it as a directed acyclic graph (DAG) of $N$ nodes $\{x_0, x_1,...,x_{N-1}\}$, each of the nodes can be regarded as a layer of neural network. An edge $E_{(i,j)}$ connects the node $x_i$ and node $x_j$, consisting of a set of candidate operations. We denote the operation space as $\mathcal{O}$, and following the original DARTS setting there are 8 candidate operations in it.

	% a candidate operation $o\in\mathcal{O}$ can be one of \textit{skip-connect, 3x3 avg-pooling, 3x3 max-pooling, 3x3 separable convolution, 5x5 separable convolution, 3x3 dilated separable convolution} or \textit{5x5 dilated separable convolution}. Especially a \textit{zero} operator is also included in $\mathcal{O}$ as a scaling factor to weight the different edges connected to the same node. According to \cite{liu2018darts}, if we search for two kinds of cells, i.e. \textit{normal cell} and \textit{reduction cell}, then the number of underlying architectures in this search space can be up to $10^{18}$.

	DARTS deals with the information flow between nodes by displacing the discrete operation selection with a continuous relaxation, which can be formulated as:
	\begin{align}
		\bar{o}^{(i,j)}(x_i) &= \sum_{o\in\mathcal{O}} p^{(i,j)}_o \cdot o^{(i,j)}(x_i) \nonumber
		\\ &= \sum_{o\in\mathcal{O}} \frac{\exp(\alpha_o^{(i,j)})}{\sum_{o'\in\mathcal{O}}\exp(\alpha_{o'}^{(i,j)})} \cdot o^{(i,j)}(x_i)\label{con:softmax}
	\end{align}
	This formula explains how a feature mapping $\bar{o}^{(i,j)}(x_i)$ on edge $E_{(i,j)}$ is computed from the previous node $x_i$. Here $p^{(i,j)}_o$ represents the contribution of each operation $o^{(i,j)}\in{\mathcal{O}}$, which is a softmax of the learnable architecture parameters $\alpha_o^{(i,j)}$. Within a cell, each immediate node $x_j$ is represented as the sum of all the feature mappings on edges connecting to it: $x_j = \sum_{i<j} \bar{o}^{(i,j)}(x_i)$.
	%Note that in each cell, there are two input nodes $x_0$ and $x_1$, and a single output node concatenates all the immediate nodes in a depth-wise manner.

	During the search stage, the optimization of network weights $w$ and architecture parameters $\alpha$ is formulated as a bi-level optimization problem \cite{anandalingam1992hierarchical}, where $w$ is optimized through minimizing the training loss $\mathcal{L}_{train}$ as a lower-level variable, and $\alpha$ is optimized through minimizing the validation loss $\mathcal{L}_{val}$ as an upper-level variable:
	\begin{align}
		\min_\alpha\quad &\mathcal{L}_{val}(w^*(\alpha), \alpha)
		\\\text{s.t.}\quad &w^*(\alpha) = \mathop{\arg\min}_w \mathcal{L}_{train}(w,\alpha)
	\end{align}
	After the architecture parameters $\alpha$ are obtained, we can derive the final searched architecture following these steps: 1) replace the mixed operation on each edge by a discrete operation $o^{(i,j)}=\arg\max_{o\in\mathcal{O}, o\neq zero}p_o^{(i,j)}$, 2) retain two edges from different predecessor nodes with the largest $\max_{o\in\mathcal{O}, o\neq zero}p_o^{(i,j)}$.

	\subsection{One-level DARTS and Its Problems}

	Since DARTS is formulated as a bi-level optimization problem, there have been many research works that pointed out DARTS is unstable and difficult to converge. For example, PDARTS \cite{chen2019progressive} suffers from the weaker search stability when they gradually grow the depth of search backbone. DARTS+ \cite{liang2019darts+} also proposes a similar approach to control the number of \textit{skip-connect} operation in the searched architecture. In a more recent work \cite{li2019stacnas}, the author concludes that bi-level DARTS is very difficult to optimize, often leads to poor results, and also suffers from high computational cost and data inefficiency.

	To reduce the instability problem of DARTS, a more direct and simpler approach is to replace the bi-level optimization by a one-level approach, where the weight parameter $w$ and the architecture parameter $\alpha$ are updated jointly and simultaneously. The update rule can be easily obtained by applying stochastic gradient descent on both the parameters as:
	\begin{align}
		w &:= w-\lambda \cdot \frac{\partial}{\partial w} \mathcal{L}_{train}(w, \alpha)
		\\\alpha &:= \alpha-\zeta \cdot \frac{\partial}{\partial \alpha} \mathcal{L}_{train}(w, \alpha)
	\end{align}
	here $\lambda$ and $\zeta$ are learning rates for $w$ and $\alpha$ respectively.

	One-level optimization has many advantages such as easier to implement, faster to converge, higher memory and data efficiency. However, if we directly apply the one-level optimization in the original DARTS search space, the performance will get worse than the bi-level way \cite{liu2018darts}.



	\subsection{The Matthew Effect and Multiple Collinearity Problem}

	Essentially, DARTS trains about $10^{18}$ child models simultaneously within a one-shot model, where different child models share the weights of the same operation. And given the same input, the similar operations will always produce correlated feature mappings. However, we also hope that the feature mappings of different operations are highly irrelevant, otherwise, the training of $\alpha$ would suffer from multiple collinearity problem \cite{li2019stacnas}, because one-level DARTS is a linear system with respect to $\alpha$.

	Besides, we observe that in the one-level DARTS, the similar parameterized operations are also greatly affected by Matthew Effect, i.e. the \textit{rich-get-richer} phenomenon \cite{adam2019understanding} previously reported in ENAS, that at least one such operation's weight can be hardly learned, resulting in generating a quite different feature mapping compared to other parameterized operations, which harms the final architecture selection in this weight sharing environment.
	%And we also observe that the training of architecture parameters of the non-parameterized operations indeed largely suffers from multiple collinearity problem, since the corresponding feature mappings are remarkably similar.

	To illustrate these phenomenons, at the end of the architecture search stage, we cluster the averaged feature mappings generated from all the seven operations (except \textit{zero}) on edge $E_{(0,2)}$. We randomly select 1000 data, from CIFAR-10 and CIFAR-100 respectively, to generate the feature mappings, and apply K-means to cluster these mappings into 3 clusters to show the similarities between them, and finally use PCA to get a two-dimensional illustration, see \fig{fig:cluster}. We can see that due to Matthew Effect, the green point locates far away from the red cluster, while the red points huddle together. The points of the non-parameterized operations are not distinguishable from each other in some cases, which leads to severe multiple collinearity problem.


	\begin{figure}[t]
		\vspace{-3pt}
		\centering
		\subfigure[C10 first]{\includegraphics[width=.24\linewidth]{img/corr_first.pdf}}
		%\hspace{5pt}
		\subfigure[C10 last]{\includegraphics[width=.24\linewidth]{img/corr_last.pdf}}
		%\hspace{-6pt}
		\subfigure[C100 first]{\includegraphics[width=.24\linewidth]{img/corr_first_cifar100.pdf}}
		%\hspace{-6pt}
		\subfigure[C100 last]{\includegraphics[width=.24\linewidth]{img/corr_last_cifar100.pdf}}
		%\vspace{-14pt}
		\caption{\small{Feature clusters generated from different operations on edge $E_{(0,2)}$. In (a) and (c) we select the edge in the first cell of one-shot model, and in (b) and (d) we select the edge in the last cell. }}\label{fig:cluster}
		%\vspace{-50pt}
		% We use 1000 data randomly sampled from CIFAR-10 and CIFAR-100 to generate the feature mappings in the one-level DARTS. Under all the cases, we can observe a single green point that represented as one of the parameterized operation, shifts far away from other parameterized ones, and we can also see the non-parameterized points locate very close to each other, hard to distinguish.
	\end{figure}


	\subsection{Grouped Operation Dropout}\label{sec:god}

	Since the existing Matthew Effect and the multiple collinearity problem in the one-level DARTS are both caused by the simultaneous updates, we propose a simple but effective training paradigm called \textit{Grouped Operation Dropout} to break the correlation by stabilizing the one-level optimization in the weight sharing one-shot model.

	Specifically, during the search stage, for each batch and each edge we randomly and independently select a subset of the operations, zeroing out their outputs, making their $\alpha$ and $w$ not be updated from the gradient. Such a paradigm mitigates the Matthew Effect among the parameterized operations since the under fitted operations have more chance to play an important role during the one-shot model training so that it can better fit the training target and benefit the $\alpha$'s learning. The multiple collinearity problem in correlated operations is also alleviated because dropout is essentially a regularization technique that reduce the degree of freedom of $\alpha$ \cite{srivastava2014dropout}.

	As the operation dropout technique plays different roles for the above issues, we analogously partition the 8 operations into two groups according to whether they are learnable, i.e. one non-parameterized group $\mathcal{O}_{np}$ includes \textit{zero, skip-connect, 3x3 avg-pooling, 3x3 max-pooling}, and one parameterized group $\mathcal{O}_p$ includes \textit{3x3 separable conv, 5x5 separable conv, 3x3 dilated separable conv, 5x5 dilated separable conv}. During the whole search procedure, for the $\mathcal{O}_p$ group, for example, we fix the dropout rate of each operation on each edge in this group to $r_p^{1/|\mathcal{O}_p|}$, where $0<r<1$ is a hyperparameter and $|\mathcal{O}_p|$ is the number of operation in $\mathcal{O}_p$, which is 4 in this case, so the hyperparameter $r$ denotes the probability of disabling all the operations in $\mathcal{O}_p$. However, we enforce at least one operation to remain in each group to further stabilize training, which is realized by resampling if the operations on some edge happen to be all dropped. If we set $r=3\times10^{-5}$, then the probability of an operation to be dropped is around $7.5\%$. During the backpropagation period, the $w$ and $\alpha$ of the dropped operations will receive no gradient.

	There are two major advantages of grouped dropout. The first is for each group we can employ different dropout rates during the search stage for achieving different objectives, like reducing the Matthew Effect in $\mathcal{O}_p$ and the multiple collinearity problem in $\mathcal{O}_{np}$. The second is by enforcing to keep at least one operation in each group, the equivalent function of each edge is always a mixture of learnable and non-learnable operations, resulting in a relatively stable training environment for the architecture parameters $\alpha$.

	\paragraph{$\alpha$-Adjust: Prevent Passive Update} Note that in DARTS, we measure the contribution $p_o^{(i,j)}$ of a certain operation $o^{(i,j)}$ on the edge $E_{(i,j)}$ via a softmax over all the learnable architecture parameters $\alpha_o^{(i,j)}$, as seen in Equation \eqref{con:softmax}. As a result, the contribution $p_o^{(i,j)}$ of the dropped operations that do not receive any gradient during the backward pass, will get changed even though their corresponding $\alpha_{o}^{(i,j)}$ remain the same. In order to prevent the passive update of the dropped operations' $p_o^{(i,j)}$, we need to adjust the value of each $\alpha_o^{(i,j)}$ after applying the gradient. Our approach is to solve for an additional term $x$ according to:
	\begin{align}
		\frac{\sum_{o\in\mathcal{O}_d}\exp(\alpha_o^{old})}{\sum_{o\in\mathcal{O}_k}\exp(\alpha_o^{old})} = \frac{\sum_{o\in\mathcal{O}_d}\exp(\alpha_o^{new})}{\sum_{o\in\mathcal{O}_k}\exp(\alpha_o^{new}+x)} \label{con:adjust}
	\end{align}
	we omit the subscript $(i,j)$, here $\mathcal{O}_d$ \& $\mathcal{O}_k$ refer to the operation set that are dropped \& kept on edge $E_{(i,j)}$, $\alpha_o^{old}$ \& $\alpha_o^{new}$ means the value before \& after backpropagation. With the additional term $x$ to adjust the value of $\alpha_o^{new}$ for $o\in\mathcal{O}_k$, the contribution $p_o^{(i,j)}$ for $o\in\mathcal{O}_d$ remains unchanged. Note that $\alpha_o^{old}=\alpha_o^{new}$ for $o\in\mathcal{O}_d$, by solving Equation \eqref{con:adjust} we get:
	\begin{align}
		x = \ln\left[\frac{\sum_{o\in\mathcal{O}_k}\exp(\alpha_o^{old})}{\sum_{o\in\mathcal{O}_k}\exp(\alpha_o^{new})}\right] \label{con:x_equa}
	\end{align}

	\paragraph{Partial-Decay: Prevent Unnecessary Weight Decay}

	L2 regularization is employed during the search stage of original DARTS, and we also find it useful in one-level optimization. But when applied with dropout, the parameters of $\mathcal{O}_d$ will be regularized even when they are dropped. So in our implementation, we apply the L2 weight decay only to those $w$ and $\alpha$ belonging to $\mathcal{O}_k$ to prevent the over-regularization.



	\section{Related Works}

	\paragraph{Weight-Sharing Differentiable NAS} It is popular in recent NAS researches to leverage the weight-sharing approach to accelerate searching. ENAS \cite{pham2018efficient} uses an RL meta-controller to explore the search space, and for each batch, it samples a single child model whose parameters are inherited from the supernet. Many follow-up works substitute the RL optimizer with a gradient-based approach like DARTS \cite{liu2018darts}, SNAS \cite{xie2018snas} and ProxylessNAS \cite{cai2018proxylessnas}. However, their search paradigms about to what extent the weights should be shared are still heuristic. DropNAS discovers a robust sharing scheme across various search spaces and datasets, demonstrating that different from the previous works, search on the one-shot model is probably stable when just a few operations are dropped. Note that the stability of weight-sharing in NAS have been studied before, such as \cite{zela2019understanding,bender2018understanding,sciuto2019evaluating,chu2019fairnas}. Different from all the previous works, we focus on the weight-sharing problems specifically in one-level DARTS, and achieve some state-of-the-art results on various benchmark datasets.


	\paragraph{Operation Dropout} Similar idea of operation dropout has been proposed in some previous works, but all of them suffer from some limitations. \cite{bender2018understanding} employs a path dropout technique into a backbone without learning $\alpha$ like DARTS, so they have to figure out the final architecture by sampling the submodels rather than ranking $\alpha$, which is inefficient and suboptimal. PDARTS \cite{chen2019progressive} improves on bi-level DARTS, and to stabilize searching they drop only the \textit{skip-connect} operation and force the final architecture to contain a certain number of \textit{skip-connect}, which is subjective and obeys the NAS motivation. Single-Path NAS \cite{stamoulis2019single} also employs operation dropout, however, they search in a simpler and restricted search space, where all the convolution operations have shared weights and just one non-learnable operation is incorporated. We propose a grouped operation dropout technique into the more complex DARTS search space suffering from both Matthew Effect and multiple collinearity problems.

	\paragraph{One-Level DARTS} Differentiable architecture search is usually conducted by bi-level optimization as first proposed in \cite{liu2018darts}. Due to its inefficiency, more and more works begin to focus on the one-level approach. \cite{xie2018snas} tries to optimize on the ENAS backbone but does not achieve competitive results, Single-Path NAS \cite{stamoulis2019single} gets one-level optimization work well by carefully designing a simpler single-path search space, where just one non-parameterized operation -- \textit{skip-connect} -- is included, and all the convolutional kernels have shared parameters. StacNAS \cite{li2019stacnas} shows that coupled with the backward variable pruning, one-level optimization in the multi-path DARTS search space can also achieve promising results, but they manually divide the NAS procedure into multiple stages. DropNAS significantly improves the one-level DARTS within just a single search stage and is also transferable to various search spaces.


	\renewcommand\tabcolsep{3.5pt}

	\begin{table}[t]
		\vspace{5pt}
		\tiny
		\centering
		\begin{tabular}{cccccc}
			\hline
			\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Architecture}}}  & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Test Error (\%)}}} & \textbf{Param} & \textbf{Search Cost} &  \textbf{Search} \\

			\multicolumn{1}{c}{}  & \multicolumn{1}{c}{} & \textbf{(M)} & \textbf{(GPU Days)} &  \textbf{Method} \\
			\hline
			NASNet-A~\cite{zoph2016neural} & $2.65$ & $3.3$ & $1800$ & RL \\
			AmoebaNet-B~\cite{real2019regularized} & $2.55 \pm 0.05$ & $2.8$ & $3150$ & evolution \\
			PNAS~\cite{liu2018progressive}$^1$ & $3.41 \pm 0.09$ & $3.2$ & $225$ & SMBO \\
			ENAS~\cite{pham2018efficient} & $2.89$ & $4.6$ & $0.5$ & RL \\
			\hline
			DARTS~\cite{liu2018darts} & $3.00$ & $3.3$ & $1.5$ & gradient \\
			SNAS~\cite{xie2018snas} & $2.85$ & $2.8$ & $1.5$ & gradient \\
			ProxylessNAS~\cite{cai2018proxylessnas}$^2$ & $2.08$& $5.7$ & $4$ & gradient \\
			P-DARTS~\cite{chen2019progressive} & $2.50$ & $3.4$ & $0.3$ & gradient \\
			DARTS+~\cite{liang2019darts+}$^3$ & $2.20(2.37 \pm 0.13)$ & $4.3$ & $0.6$ & gradient \\
			StacNAS~\cite{li2019stacnas} & $2.33(2.48 \pm 0.08)$ & $3.9$ & $0.8$ & gradient \\
			ASAP~\cite{noy2019asap} & $2.49 \pm 0.04$ & $2.5$ & $0.2$ & gradient \\
			PC-DARTS~\cite{xu2019pc} & $2.57 \pm 0.07$ & $3.6$ & $0.1$ & gradient \\
			\hline
			\textbf{DropNAS}$^4$ & $\mathbf{2.26} (2.58 \pm 0.14)$ & $4.1$ & $0.6$ & gradient \\
			\textbf{DropNAS (Augmented)}{$^5$} & $\mathbf{1.88}$ & $4.1$ & $0.6$ & gradient \\
			\hline
		\end{tabular}
		\caption{Performance of different architectures on CIFAR-10. PNAS$^1$ reports the result without cutout augmentation. ProxylessNAS$^2$ uses a search space different from DARTS. DARTS+$^3$ trains the evaluation model for 2,000 epochs, while others just train 600 epochs. Our DropNAS$^4$ reports both the mean and standard deviation with 8 seeds, \textbf{without increasing the training epochs or channels}. DropNAS (Augmented)$^5$ denotes training with AutoAugment and 1,200 epochs.}
		\label{tab:cifar10_results}
	\end{table}



	\begin{table}[t]
		\tiny
		\centering
		\begin{tabular}{cccccc}
			\hline
			\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Architecture}}}  & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Test Error (\%)}}} & \textbf{Param} & \textbf{Search Cost} &  \textbf{Search} \\

			\multicolumn{1}{c}{}  & \multicolumn{1}{c}{} & \textbf{(M)} & \textbf{(GPU Days)} &  \textbf{Method} \\
			\hline
			DARTS~\cite{liu2018darts}$^1$ & $17.76$ & $3.3$ & $1.5$ & gradient \\
			P-DARTS~\cite{chen2019progressive} & $15.92$ & $3.6$ & $0.3$ & gradient \\
			DARTS+~\cite{liang2019darts+}$^2$ & $14.87(15.45 \pm 0.30)$ & $3.9$ & $0.5$ & gradient \\
			StacNAS~\cite{li2019stacnas} & $15.90(16.11 \pm 0.2)$ & $4.3$ & $0.8$ & gradient \\
			ASAP~\cite{noy2019asap}$^1$ & $15.6$ & $2.5$ & $0.2$ & gradient \\
			\hline
			\textbf{DropNAS}$^3$ & $16.39 (16.95 \pm 0.41)$ & $4.4$ & $0.7$ & gradient \\
			\textbf{DropNAS (Augmented)}{$^4$} & $\mathbf{14.10}$ & $4.4$ & $0.7$ & gradient \\
			\hline
		\end{tabular}
		\caption{Results of different architectures on CIFAR-100. The results  denoted with $^1$ use the architectures found on CIFAR-10. The subscript $^2,^3$ and $^4$ have the same meaning as in Table \ref{tab:cifar10_results}.}
		\label{tab:cifar100_results}
		%\vspace{-10pt}
	\end{table}



	\section{Benchmark}

	\subsection{Datasets}

	To benchmark our grouped operation dropout algorithm, extensive experiments are carried out on CIFAR-10, CIFAR-100 and ImageNet.

	Both the CIFAR-10 and CIFAR-100 datasets contain 50K training images and 10K testing images, and the resolution of each image is $32\times 32$. All the images are equally partitioned into 10/100 categories in CIFAR-10/100.

	ImageNet is a much larger dataset consisting of 1.3M images for training and 50K images for testing, equally distributed among 1,000 classes. In this paper, we use ImageNet to evaluate the transferability of our architectures found on CIFAR-10/100. We follow the conventions of \cite{liu2018darts} that consider the mobile setting to fix the size of the input image to $224\times224$ and limit the multiply-add operations to be no more than 600M.

	\subsection{Implementation Details}

	\paragraph{Architecture Search} \label{sec:search}

	As we have mentioned before, we leverage the DARTS search space with the same 8 candidate operations. Since we use one-level optimization, the training images do not need to be split for another validation set, so the architecture search is conducted on CIFAR-10/100 with all the training images on a single Nvidia Tesla V100. We use 14 cells stacked with 16 channels to form the one-shot model, train the supernet for 76 epochs with batch size 96, and pick the architecture discovered in the final epoch. The model weights $w$ are optimized by SGD with initial learning rate 0.0375, momentum 0.9, and weight decay 0.0003, and we clip the gradient norm of $w$ to be less than 3 for each batch. The architecture parameters $\alpha$ are optimized by Adam, with initial learning rate 0.0003, momentum (0.5, 0.999) and weight decay 0.001. Dropout rate $r$ is fixed to $3\times10^{-5}$.








	\begin{figure}[t]
		%\vspace{-15pt}
		\centering
		\subfigure{\includegraphics[width=.49\linewidth]{img/rate_acc_cifar10.pdf}}
		%\hspace{5pt}
		\subfigure{\includegraphics[width=.49\linewidth]{img/rate_acc_cifar100.pdf}}
		%\hspace{-6pt}
		%\hspace{-14pt}
		\caption{\small{The impact of operation dropout rate reflected on the stand-alone model accuracy. The red error bar shows the standard deviation of 8 repeated experiments. }}\label{fig:rate}
		% The best rate $r=3\times10^{-5}$ is transferable on different datasets.
		\vspace{-10pt}
	\end{figure}



	\begin{figure}[t]
		%\vspace{-10pt}
		\centering
		\subfigure[CIFAR-10 normal cell]{\includegraphics[width=.47\linewidth]{img/normal_cifar10.pdf}}
		%\vspace{-10pt}
		\subfigure[CIFAR-100 normal cell]{\includegraphics[width=.52\linewidth]{img/normal_cifar100.pdf}}
		%\vspace{-10pt}
		\subfigure[CIFAR-10 reduction cell]{\includegraphics[width=.99\linewidth]{img/reduction_cifar10.pdf}}
		%\vspace{-6pt}
		\subfigure[CIFAR-100 reduction cell]{\includegraphics[width=.99\linewidth]{img/reduction_cifar100.pdf}}
		%\hspace{-14pt}
		\caption{\small{The found architectures on CIFAR-10 and CIFAR-100}}\label{fig:arch}
		%\vspace{-10pt}
	\end{figure}

	\paragraph{Architecture Evaluation}

	On CIFAR-10 and CIFAR-100, to fairly evaluate the discovered architectures, neither the initial channels nor the training epochs are increased for the evaluation network, compared with DARTS. 20 cells are stacked to form the evaluation network with 36 initial channels. The network is trained on a single Nvidia Tesla V100 for 600 epochs with batch size 192. The network parameters are optimized by SGD with learning rate 0.05, momentum 0.9 and weight decay 0.0003, and the gradient is clipped in the same way as in the search stage. The data augmentation method Cutout and an auxiliary tower with weight 0.4 are also employed as in DARTS. To exploit the potentials of the architectures, we additionally use AutoAugment to train the model for 1,200 epochs. The best architecture discovered are represented in \fig{fig:arch} and their evaluation results are shown in Table \ref{tab:cifar10_results} and \ref{tab:cifar100_results}. We can see that the best architecture discovered by DropNAS achieves the state-of-the-art test error $2.26\%$ on CIFAR-10, and on CIFAR-100 DropNAS still works well compared to the original DARTS, and largely surpasses the one-level version which prefers to ending up with many \textit{skip-connect} in the final architecture if directly searched on CIFAR-100.

	To test the transferability of our selected architecture, we adopt the best architecture found on CIFAR-10 and CIFAR-100 to form a 14-cell, 48-channel evaluation network to train on ImageNet. The network is trained for 600 epochs with batch size 2048 on 8 Nvidia Tesla V100 GPUs, optimized by SGD with initial learning rate 0.8, momentum 0.9, weight decay $3\times 10^{-5}$, and gradient clipping 5.0. The additional enhancement approaches that we use include AutoAugment, mixup, SE module, auxiliary tower with loss weight 0.4, and label smoothing with $\epsilon=0.1$. Table \ref{tab:imagenet_results} shows that the architecture found by DropNAS is transferable and obtains encouraging result on ImageNet.










	\begin{table}[t]
		\vspace{5pt}
		\centering
		\tiny
		\begin{tabular}{cccccc}
			\hline
			\multirow{2}{*}{\textbf{Architecture}} & \multicolumn{2}{c}{\textbf{Test Err. (\%)}} & \textbf{Params}  & \textbf{Search} & \textbf{Search} \\
			\cline{2-3}
			& \textbf{Top-1} & \textbf{Top-5} & \textbf{(M)} & \textbf{Days} & \textbf{Method}   \\
			\hline
			%ShuffleNet-V2 (2x)~\cite{ma2018shufflenet} & $25.1$ & - & $7.4$ & - & manual \\
			%\hline
			NASNet-A~\cite{zoph2016neural} & $26.0$ & $8.4$ & $5.3$ & $1800$ & RL \\
			%AmoebaNet-C~\cite{real2019regularized} & $24.3$ & $7.6$ & $6.4$ & $3150$ & RL \\
			PNAS~\cite{liu2018progressive} & $25.8$ & $8.1$ & $5.1$ & $588$ & SMBO \\
			%MnasNet-92~\cite{tan2019mnasnet} & $25.2$ & $8.0$ & $4.4$ & - & RL \\
			EfficientNet-B0~\cite{tan2019efficientnet} & $23.7$ & $6.8$ & $5.3$ & - & RL \\
			\hline
			DARTS~\cite{liu2018darts} & $26.7$ & $8.7$ & $4.7$ & $4.0$ & gradient \\
			SNAS (mild)~\cite{xie2018snas} & $27.3$ & $9.2$ & $4.3$ & $1.5$ & gradient \\
			ProxylessNAS~\cite{cai2018proxylessnas}$^\dag$$^*$ & $24.9$ & $7.5$ & $7.1$ & $8.3$ & gradient \\
			P-DARTS (C10)~\cite{chen2019progressive} & $24.4$ & $7.4$ & $4.9$ & $0.3$ & gradient \\
			ASAP~\cite{noy2019asap} & $26.7$ & - & - & $0.2$ & gradient \\
			XNAS~\cite{nayman2019xnas} & $24.0$ & - & $5.2$ & $0.3$ & gradient \\
			PC-DARTS~\cite{xu2019pc}$^\dag$ & $24.2$ & $7.3$ & $5.3$ & $3.8$ & gradient \\
			ScarletNAS\cite{chu2019scarletnas}$^{\dag}$$^*$ & $23.1$ & $6.6$ & $6.7$ & $10$ & gradient \\
			DARTS+\cite{liang2019darts+}$^{\dag}$ & $23.9$ & $7.4$ & $5.1$ & $6.8$ & gradient \\
			StacNAS\cite{li2019stacnas}$^{\dag}$ & $24.3$ & $6.4$ & $5.7$ & $20$ & gradient \\
			Single-Path NAS \cite{stamoulis2019single}$^{\dag}$$^*$ & $25.0$ & $7.8$ & $-$ & $0.16$ & gradient \\
			\hline
			\textbf{DropNAS (CIFAR-10)} & $\mathbf{23.4}$ & $\mathbf{6.7}$ & $5.7$ & $ 0.6 $ & gradient \\
			\textbf{DropNAS (CIFAR-100)} & $\mathbf{23.5}$ & $\mathbf{6.8}$ & $6.1$ & $ 0.7 $ & gradient \\
			\hline
		\end{tabular}
		\caption{Results of different architectures on ImageNet. The results denoted with $^\dag$ use the architectures directly searched on ImageNet, and those denoted with $^*$ use the backbone different from DARTS.}
		\label{tab:imagenet_results}
	\end{table}




	\begin{figure}[t]
		\vspace{-5pt}
		\centering
		\subfigure[C10 first]{\includegraphics[width=.24\linewidth]{img/corr_first(drop).pdf}}
		%\hspace{5pt}
		\subfigure[C10 last]{\includegraphics[width=.24\linewidth]{img/corr_last(drop).pdf}}
		%\hspace{-6pt}
		\subfigure[C100 first]{\includegraphics[width=.24\linewidth]{img/corr_first_cifar100(drop).pdf}}
		%\hspace{-6pt}
		\subfigure[C100 last]{\includegraphics[width=.24\linewidth]{img/corr_last_cifar100(drop).pdf}}
		%\hspace{-14pt}
		\caption{\small{Feature clusters of DropNAS from different operations on $E_{(0,2)}$, generated in the same way as \fig{fig:cluster}.}} \label{fig:cluster_drop}
		\vspace{-10pt}
	\end{figure}



	\section{Diagnostic Experiments}

	\subsection{Impact of Dropout Rate}

	In DropNAS we introduce a new hyperparameter, i.e. the operation dropout rate $r$, whose value has a strong impact on the results, since a higher dropout rate results in a lower correlation between the operations. To demonstrate its significance, we repeat the search and evaluation stages with varying operation dropout rates and report the stand-alone model accuracy in \fig{fig:rate}. The best results are achieved when $r=3\times10^{-5}$ on both datasets, which indicates that the found best dropout rate is transferable to different datasets. Note that the expectation of the number of dropped operations on each edge is just $0.15$ when $r=3\times10^{-5}$, demonstrating that architecture search on one-shot model achieves the best performance when only a few operations are dropped in each batch.


	\subsection{Feature Clusters in DropNAS}

	For comparison we again draw the feature clusters in DropNAS with $r=3\times10^{-5}$, following the same way in \fig{fig:cluster}. The results are plotted in \fig{fig:cluster_drop}.

	It is significant that the point of parameterized operation no longer shifts away from its similar partner, and there is no cluster containing only one single point anymore. So we claim that the severe Matthew Effect existing in the one-level DARTS has been greatly reduced by DropNAS.

	Multiple collinearity problems can be naturally alleviated by applying DropNAS to reduce the degree of freedom in $\alpha$'s learning. To our surprise, the points of non-parameterized operations also become more scattered compared to \fig{fig:cluster}, which may due to more distinguishable feature mappings are learned in DropNAS.




	\subsection{Performance on Other Search Space}

	We are also interested in the adaptability of DropNAS in other search spaces. We purposely design two search spaces: in the first space we replace the original \textit{3x3 avg-pooling} and \textit{3x3 max-pooling} operations by \textit{skip-connect}, so that the operation group $\mathcal{O}_{np}$ becomes highly correlated and suffers from severe multiple collinearity problem; And in the second space we remove the \textit{3x3 avg-pooling} and \textit{3x3 max-pooling} operations in $\mathcal{O}_{np}$, leading to more intense competition among $o\in\mathcal{O}_{p}$. We again search on CIFAR-10 and evaluate the found architectures, report the mean accuracy and standard deviations of 8 repeated runs.

	The results shown in Table \ref{tab:diff_space} demonstrates that DropNAS is robust across different search spaces in different datasets.

	%In the 3-skip space, the architectures discovered by one-level DARTS are full of convolutional operations without any \textit{skip-connect}, since the three same operations are highly correlated and disperse the significance of each other. DropNAS reduces such correlation and all the discovered architectures have at least one \textit{skip-connect}. And in the 1-skip search space, DropNAS again maintains the high performance. The number of \textit{skip-connect} found by one-level DARTS varies from 3 to 5 due to the correlation between similar convolutions.


	\subsection{Impact of Dropout Rates in Different Groups}

	As we mentioned in Section \ref{sec:god}, one advantage of grouping in DropNAS is that we can apply different dropout rates to different operation groups. However, our architecture search is actually conducted with $r$ fixed to $3\times 10^{-5}$ for both $\mathcal{O}_p$ and $\mathcal{O}_{np}$. In fact, we have assigned $\mathcal{O}_p$ and $\mathcal{O}_{np}$ with different dropout rates around $3\times 10^{-5}$, and the results are shown in Table \ref{tab:diff_rates}, which means the best performance is achieved when the two groups share exactly the same rate.

	\renewcommand{\arraystretch}{1.2}


	\begin{table}[]
		%\vspace{-5pt}
		\tiny
		\begin{tabular}{cccc}
			\hline
			\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Dataset}}} & \multicolumn{1}{c}{\multirow{1}{*}{\textbf{Search}}} & \multicolumn{2}{c}{\textbf{Test Error (\%)}} \\
			\cline{3-4}
			\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{Space}} & \textbf{DropNAS} & \textbf{one-level DARTS} \\
			\hline
			\multicolumn{1}{c}{\multirow{3}{*}{CIFAR-10}} &  3-skip & 2.68$\pm$0.10   & 3.19$\pm$0.18              \\
			\multicolumn{1}{c}{} & 1-skip        & 2.67$\pm$0.11    & 2.85$\pm$0.12         \\
			\multicolumn{1}{c}{} & original                   & 2.58$\pm$0.14    & 2.90$\pm$0.16           \\
			\hline
			\multicolumn{1}{c}{\multirow{3}{*}{CIFAR-100}} & 3-skip & 16.97$\pm$0.35      & 18.00$\pm$0.34         \\
			\multicolumn{1}{c}{} & 1-skip                   & 16.47$\pm$0.19    & 17.73$\pm$0.25           \\
			\multicolumn{1}{c}{} & original                   & 16.95$\pm$0.41    & 17.27$\pm$0.36           \\
			\hline
		\end{tabular}
		\caption{\small{The performance of DropNAS and one-level DARTS across different search spaces on CIFAR-10/100.}}\label{tab:diff_space}
	\end{table}



	\begin{table}[]
		\scriptsize
		\centering
		\begin{tabular}{|l|l|l|l|}
			\hline
			\diagbox{$\mathcal{O}_{np}$}{$\mathcal{O}_{p}$} & $1\times10^{-5}$ &
			$3\times10^{-5}$        & $1\times10^{-4}$ \\
			\hline
			$1\times10^{-5}$ & 2.60$\pm$0.16   & 2.72$\pm$0.04          & 2.64$\pm$0.12   \\
			\hline
			$3\times10^{-5}$ & 2.64$\pm$0.11   & \textbf{2.58}$\pm$\textbf{0.14} & 2.69$\pm$0.05   \\
			\hline
			$1\times10^{-4}$ & 2.65$\pm$0.07   & 2.69$\pm$0.10          & 2.63$\pm$0.16   \\
			\hline
		\end{tabular}
		\caption{\small{The test error of DropNAS on CIFAR-10 when the operation groups $\mathcal{O}_{p}$ and $\mathcal{O}_{np}$ are applied with different dropout rates. The above results are obtained over 8 different seeds.}} \label{tab:diff_rates}
	\end{table}




	\subsection{Performance Correlation between Stand-Alone Model and Architecture Parameters}

	DropNAS is supposed to break the correlation between the operations, so that the architecture parameters $\alpha$ can represent the real importance of each operation, and then we can easily select the best the architecture by ranking $\alpha$. \fig{fig:corr_acc} shows the correlation between the architectures and their corresponding $\alpha$ on two representative edges in normal cell, $E_{(0,2)}$ and $E_{(4,5)}$, which are the fisrt and the last edge within the cell. We claim that the $\alpha$ learned by DropNAS has a vigorous representative power of the accuracy of the stand-alone model, since the correlation coefficient between them is 0.902 on $E_{(0,2)}$, largely surpassing that of DARTS (0.2, reported in \cite{li2019stacnas}), and 0.352 on $E_{(4,5)}$, where the choice of a specific operation is less significant.

	\begin{figure}[t]
		\centering
		\subfigure[on edge $E_{(0,2)}: 0.902$]{\includegraphics[width=.49\linewidth]{img/acc_corr_02.pdf}}
		%\hspace{5pt}
		\subfigure[on edge $E_{(4,5)}: 0.352$]{\includegraphics[width=.49\linewidth]{img/acc_corr_45.pdf}}
		\vspace{-10pt}
		\caption{\small{Correlation coefficients between the accuracy of stand-alone model and their corresponding $\alpha$. The results are obtained by first searching on CIFAR-10, figuring out the best architecture, then generating other 6 architectures by replacing the operation on edges $E_{(0,2)}$ and $E_{(4,5)}$ in the normal cell with other $o\in\mathcal{O}$, and finally the corresponding stand-alone models are trained from scratch.}}\label{fig:corr_acc}
		%\vspace{-10pt}
	\end{figure}


	\begin{table}[]
		\vspace{-10pt}
		\scriptsize
		\centering
		\begin{tabular}{ccccc}
			\multicolumn{1}{c}{\multirow{2}{*}{}} & \multicolumn{4}{c}{\textbf{Test Err. (\%)}} \\
			\cline{2-5}
			\multicolumn{1}{c}{} & \textbf{DropNAS} & \textbf{No $\alpha$-adjust} & \textbf{No partial-decay} & \textbf{No grouping} \\
			\hline
			\textbf{CIFAR-10} & \textbf{2.58}$\pm$\textbf{0.14} & 2.75$\pm$0.08 & 2.71$\pm$0.06 & 2.74$\pm$0.11
			\\
			\textbf{CIFAR-100} & \textbf{16.95}$\pm$\textbf{0.41} & 17.40$\pm$0.22 & 17.62$\pm$0.37 & 17.98$\pm$0.33
			\\
			\hline
		\end{tabular}
		\caption{\small{Ablation study on CIFAR-10/100, averaged over 8 runs.}}\label{tab:ablation}
	\end{table}


	\subsection{Ablation Study}

	To show the techniques we proposed in Section \ref{sec:god} really improve the DropNAS performance, we further conduct experiments for DropNAS with each of the techniques disabled. The results in Table \ref{tab:ablation} show that each components of DropNAS is indispensable for achieving a good performance.


	\section{Conclusion}

	In this paper, we propose the simple but effective DropNAS algorithm, where the grouped operation dropout is introduced to avoid the Matthew Effect and multiple collinearity problem in one-level DARTS. We conduct sufficient experiments for DropNAS on various benchmark datasets, and show competitive or even better results compared with other state-of-the-art gradient-based NAS methods. It should be noticed that in other search spaces DropNAS is able to discover quite meaningful architectures, and the best dropout rate of DropNAS is also transferable in different datasets, demonstrating its strong applicability in a wider range of tasks.







	%% The file named.bst is a bibliography style file for BibTeX 0.99c
	\bibliographystyle{named}
	\bibliography{ijcai20}

\end{document}

